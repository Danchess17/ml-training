### Решение
1) Data Preproccesing: Для начала было замечено, что все фотографии были различных размеров. Для подачи в модель нужны данные одного размера, поэтому было решено сделать padding. Было вычислено, что максимальная ширина и высота = 500, поэтому все фотографии я увеличил бекграундом до размера (500, 500). Была идея уменьшить после этого изображение до (100, 100).
4) Model Building and Training: Для построения и обучения модели была использована библиотека Keras (внутри tensorflow). 
Можно было также использовать pytorch. Еще легче было бы использовать готовые натренированные модели как показано в segmentation_model.py. Был вариант и посложнее: писать обучение самому. Я реализовал Unet-архитектуру модели, можно было выбрать и другую.

P.S.: из-за большого тренировочного датасета (особенно после паддинга) модель не смогла натренироваться из-за превышения затрат по оперативной памяти. 
Были перепробованы различные методы от ```keras.backend.clear_session()``` до простого уменьшения тренировочного датасета ```X_train[:2000], y_train[:2000]``` и уменьшения ```batch_size```. Но ничего из этого не привело к успеху...

### Структура проекта
1) preprocessing.py - подготовка датасета для подачи в модель (папка Pascal-part должна лежать на том же уровне, что и питоновские файлы)
2) model.py - реализация самой модели на Keras
3) training.py - обучение модели на тренировочном и валидационном датасете
4) build_and_train.py - смердженная вторая и третья проги
5) main.ipynb (1) - это последняя наиболее полная версия юпитер-ноутбука, чья цель была собрать все первые три программы воедино в красивом виде, но она падает из-за превышения использованной RAM (сами по себе main.ipynb и training.py падают по той же причине, процессор убивает запуск программы)
6) остальные файлы - незаконченные мысли автора по поводу использования не-Keras подхода

### Идеи 
1) Использовать не Keras из TensorFlow, а Ignite в Pytorch (судя по обсуждениям в гитхабе в Keras есть определенные проблемы с памятью)
2) Использовать другую архитектуру нейронки
3) Поварьировать гиперпараметры тренировки: поменять оптимизитатор, количество эпох, посмотреть на то, как ведет себя модель
4) Написать свою тренировку модели и построить графики падения Loss и увеличения метрики от количества эпох для тренировочной и валидационной выборок
5) Последить за другими метриками типа Hausdorff Distance, Dice coefficient или Saliency Metrics
