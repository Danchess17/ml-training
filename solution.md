### Решение
1) Data Preproccesing: Для начала было замечено, что все фотографии были различных размеров. Для подачи в модель нужны данные одного размера, поэтому я решил сделать padding. Я вычислил, что максимальная ширина и высота это 500, и все фотографии добил бекграундом до размера (500, 500). Была идея уменьшить после этого изображение до (100, 100).
4) Model Building and Training: Для построения и обучения модели была использована библиотека Keras (внутри tensorflow). 
Можно было также использовать pytorch. Еще легче было бы использовать готовые натренированные модели как показано в segmentation_model.py. Был вариант и посложнее: писать обучение самому. Я реализовал Unet-архитектуру модели, можно было выбрать и другую.

P.S.: из-за большого тренировочного датасета (особенно после паддинга) модель не смогла натренироваться из-за превышения затрат по оперативной памяти. 
Были перепробованы различные методы от keras.backend.clear_session() до простого уменьшения тренировочного датасета X_train[:1000], y_train[:1000]. Но ничего из этого не привело к успеху...
