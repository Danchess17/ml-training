### Решение
1) Data Preproccesing: Для начала было замечено, что все фотографии были различных размеров. Для подачи в модель нужны данные одного размера, поэтому я решил сделать padding. Я вычислил, что максимальная ширина и высота это 500, и все фотографии добил бекграундом до размера (500, 500). Была идея уменьшить после этого изображение до (100, 100).
4) Model Building and Training: Для построения и обучения модели была использована библиотека Keras (внутри tensorflow). 
Можно было также использовать pytorch. Еще легче было бы использовать готовые натренированные модели как показано в segmentation_model.py. Был вариант и посложнее: писать обучение самому. Я реализовал Unet-архитектуру модели, можно было выбрать и другую.

P.S.: из-за большого тренировочного датасета (особенно после паддинга) модель не смогла натренироваться из-за превышения затрат по оперативной памяти. 
Были перепробованы различные методы от ```keras.backend.clear_session()``` до простого уменьшения тренировочного датасета ```X_train[:2000], y_train[:2000]``` и уменьшения ```batch_size```. Но ничего из этого не привело к успеху...

### Структура проекта
1) preprocessing.py - подготовка датасета для подачи в модель (папка Pascal-part должна лежать на том же уровне, что и питоновские файлы)
2) model.py - реализация самой модели на Keras
3) training.py - обучение модели на тренировочном и валидационном датасете
4) build_and_train.py - смердженная вторая и третья проги
5) main.ipynb - это юпитер-ноутбук, чья цель была собрать все первые три программы воедино в красивом виде, но он падает из-за превышения использованной RAM (сама по себе training.py падает по той же причине)
6) остальные файлы - не нужны

### Идеи 
1) Использовать не Keras из TensorFlow, а Ignite в Pytorch (судя по обсуждениям в гитхабе в Keras есть определенные проблемы с памятью)
2) Использовать другую архитектуру нейронки
3) Написать свою тренировку модели
